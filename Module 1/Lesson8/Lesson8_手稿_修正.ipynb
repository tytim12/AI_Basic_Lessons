{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集：https://github.com/fatecbf/toutiao-text-classfication-dataset/blob/master/toutiao_cat_data.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索引擎的三个步骤：\n",
    "- 自动下载网页(网页爬虫)\n",
    "- 建立索引\n",
    "- 度量网页质量(pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./toutiao_cat_data.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6552368441838272771_!_101_!_news_culture_!_发酵床的垫料种类有哪些？哪种更好？_!_\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d.split('_!_') for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['id', 'code', 'category', 'content', 'keys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑不动就减小数据集\n",
    "df = df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6551700932705387022</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>京城最值得你来场文化之旅的博物馆</td>\n",
       "      <td>保利集团,马未都,中国科学技术馆,博物馆,新中国\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6552368441838272771</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>发酵床的垫料种类有哪些？哪种更好？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6552407965343678723</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>上联：黄山黄河黄皮肤黄土高原。怎么对下联？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6552332417753940238</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>林徽因什么理由拒绝了徐志摩而选择梁思成为终身伴侣？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6552475601595269390</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>黄杨木是什么树？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id code      category                    content  \\\n",
       "0  6551700932705387022  101  news_culture           京城最值得你来场文化之旅的博物馆   \n",
       "1  6552368441838272771  101  news_culture          发酵床的垫料种类有哪些？哪种更好？   \n",
       "2  6552407965343678723  101  news_culture      上联：黄山黄河黄皮肤黄土高原。怎么对下联？   \n",
       "3  6552332417753940238  101  news_culture  林徽因什么理由拒绝了徐志摩而选择梁思成为终身伴侣？   \n",
       "4  6552475601595269390  101  news_culture                   黄杨木是什么树？   \n",
       "\n",
       "                         keys  \n",
       "0  保利集团,马未都,中国科学技术馆,博物馆,新中国\\n  \n",
       "1                          \\n  \n",
       "2                          \\n  \n",
       "3                          \\n  \n",
       "4                          \\n  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_DOCUMENTS = len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 2.050 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6551700932705387022</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>京城 最 值得 你 来场 文化 之旅 的 博物馆</td>\n",
       "      <td>保利集团,马未都,中国科学技术馆,博物馆,新中国\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6552368441838272771</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>发酵 床 的 垫料 种类 有 哪些 ？ 哪 种 更好 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6552407965343678723</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>上联 ： 黄山 黄河 黄皮肤 黄土高原 。 怎么 对 下联 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6552332417753940238</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>林徽因 什么 理由 拒绝 了 徐志摩 而 选择 梁思成 为 终身伴侣 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6552475601595269390</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>黄杨木 是 什么 树 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id code      category  \\\n",
       "0  6551700932705387022  101  news_culture   \n",
       "1  6552368441838272771  101  news_culture   \n",
       "2  6552407965343678723  101  news_culture   \n",
       "3  6552332417753940238  101  news_culture   \n",
       "4  6552475601595269390  101  news_culture   \n",
       "\n",
       "                                content                        keys  \n",
       "0              京城 最 值得 你 来场 文化 之旅 的 博物馆  保利集团,马未都,中国科学技术馆,博物馆,新中国\\n  \n",
       "1          发酵 床 的 垫料 种类 有 哪些 ？ 哪 种 更好 ？                          \\n  \n",
       "2       上联 ： 黄山 黄河 黄皮肤 黄土高原 。 怎么 对 下联 ？                          \\n  \n",
       "3  林徽因 什么 理由 拒绝 了 徐志摩 而 选择 梁思成 为 终身伴侣 ？                          \\n  \n",
       "4                          黄杨木 是 什么 树 ？                          \\n  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'] = df.content.apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建字典\n",
    "vocab = []\n",
    "for document in df.content:\n",
    "    vocab.extend(document.split())\n",
    "\n",
    "# 总单词数目\n",
    "NUM_TREMS = len(vocab)\n",
    "\n",
    "vocab = dict(Counter(vocab))\n",
    "# 根据词频排序\n",
    "vocab = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "# 转换成字典\n",
    "vocab = dict(vocab)\n",
    "# 过滤少于5000词频的单词\n",
    "vocab = {k:v for k,v in vocab.items() if v>100}\n",
    "# word2id id2word\n",
    "word2id = {k:v for v,k in enumerate(vocab)}\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "# 字典数量\n",
    "NUM_WORDS = len(vocab)\n",
    "NUM_WORDS, # vocab.keys(), word2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的索引就是对每一个文档(网页)创建一个很长的二进制数字，每个数字表示其对应的关键字是否在文档当中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "way1\n",
    "\n",
    "         文档1  文档2 ...\n",
    "关键字1    1      0\n",
    "关键字2    0      1\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(x):\n",
    "    \"\"\"\n",
    "    将字符串转换成id list\n",
    "    \"\"\"\n",
    "    index = [0] * NUM_WORDS\n",
    "    words = x.split()\n",
    "    for word in words:\n",
    "        i = word2id.get(word, -1)\n",
    "        if i == -1: continue\n",
    "        index[i] = 1\n",
    "    return index\n",
    "\n",
    "\n",
    "# 验证一下是否是想要的结果\n",
    "r = generate_index('京城 最 值得 你 来场 文化 之旅 的 博物馆')\n",
    "# [id2word[int(w)] for w in \"\"\"35\n",
    "# 203\n",
    "# 7\n",
    "# 394\n",
    "# 2002\n",
    "# 2\n",
    "# 2180\"\"\".split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ps: 需要运行很长时间，占用很大内存条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = df.content.apply(generate_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "way2\n",
    "       关键字1  关键字2 关键字3 ...\n",
    "文档1    1        0       0\n",
    "文档2    0        1       1\n",
    "...\n",
    "```\n",
    "将其转置一下就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 50000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_index = np.array(all_index.tolist()).T\n",
    "all_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样会生成一个表，即索引表：表的每一行对应一个关键字，而每一个关键字后面跟着一组数字，表示哪些文档包含这个关键字。  \n",
    "这时候，当我们查询某个关键字的时候，不再需要将整个文档集合从头到尾查看一遍，只需要查找这个表就知道应该取哪些文档中去搜索就可以了。  \n",
    "这和我们生活中的书籍的目录是非常相似的。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们想要查询多个关键字的文档时候，也只需要将两个关键字对应的索引向量进行一个布尔运算(and)，便能将所有出现两个关键字的文档检索出来。  \n",
    "计算机对于布尔运算的计算速度是非常快的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如互联网有$100亿^3$($10^{10}$)个有用的网页，再假设词汇表有30万个单词(肯定偏小)，那么索引的大小至少是100亿x30万=3000万亿。  \n",
    "假如我们去掉一些只出现一个网页(文档)中的词，取百分之一，那也有30万亿。  \n",
    "\n",
    "不仅这样，索引除了保存关键字信息，还要有一些其他信息需要保存，比如词出现的位置啦，次数啦，  \n",
    "这些数据一台服务器是村放不下的，所以通常把索引切片，放到不同服务器上，这就和服务器的集群，分布式很相关了。\n",
    "\n",
    "将数据存到不同的服务器上，就会引申出很多其它问题，比如，如何保证数据的一致性(数据脏读等等)，如何降低服务期间的通信成本，否则分布式效率还不如一台服务器效果高。\n",
    "\n",
    "总之，由搜索引擎的索引可以扩展到很多很深的知识，这些知识偏向工程化，但工程化的问题不管多么复杂，至少在索引的原理上，还是非常简单的。类似目录，等价于布尔运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 度量网页质量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google的pagerank方法是常用的网页质量度量方法，在NLP领域中也是比较重要的一个方法。由于此数据集不适用于pagerank方法，这里就不再展开。  \n",
    "仅做简单介绍，有兴趣的同学可自行深入理解研究，或在开课吧NLP课程中进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在掌握\n",
    "- 数据下载\n",
    "- 建立索引\n",
    "- 度量网页\n",
    "\n",
    "后，我们再来看下搜索引擎的其它考量：\n",
    "1. 索引要完善，巧妇难为无米之炊，算法再好，要找的答案不再候选答案集合中，那都是徒劳的。\n",
    "2. 网页质量的度量，一些八卦性质的网站pagerank值比较高，但权威性质就比较低\n",
    "3. 用户偏好，不同的用户对结果的评判标准不一样，个性化推荐(深入就是走向推荐的方向了)\n",
    "4. 确定查询和网页的相关性\n",
    "\n",
    "OK，我们来看怎么确定网页和查询的相关性的度量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定网页和查询的相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们常用词频(Term Frequency)的概念来表示：  \n",
    "$$tf = \\frac{单词的出现频率}{网页的总字数}$$\n",
    "\n",
    "所以，一个文档中的单词出现的词频可以表示为：$TF_1, TF_2, ...$  \n",
    "那么假如给定关键字$w_1, w_2$，可以计算每个文档中两个关键字的词频，记为$TF_1, TF_2$  \n",
    "\n",
    "比如 `北京` `首都`   在文档 `可爱 的 北京 是 祖国 的 首都， 北京 是 个 美丽 的 城市` 和 文档`他 喊道 北京 北京 我爱你， 北京 北京 我 来了， 首都 北京 我 来 了`\n",
    "词频分别是\n",
    "[2/13, 1/13]\n",
    "[5/14, 1/14]\n",
    "\n",
    "所以，我们可以把查询关键字和文档的相关性的度量表示为：\n",
    "$TF_1+TF_2$\n",
    "\n",
    "当有N个关键字，M个文档时候，第i个文档的相似度可以记为：\n",
    "\n",
    "$$sim(q, d_i) = TF_1+TF_2+....TF_N, (q=w_1,w_2,...w_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京', '的', '房价', '多少', '？']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 北京的房价多少\n",
    "query = '北京的房价多少？'\n",
    "#query = '特朗普是什么的？'\n",
    "query = list(jieba.cut(query))\n",
    "\n",
    "# 过滤不再字典的\n",
    "query = [word for word in query if word in vocab]\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每个关键字的TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find document id that contains all keys: []\n"
     ]
    }
   ],
   "source": [
    "def querybykeys(query):\n",
    "    finds = np.array([1] * NUM_DOCUMENTS)\n",
    "    for key in query:\n",
    "        finds += all_index[word2id[key]] # 利用索引快速找到包含当前关键字的数据\n",
    "        finds = finds==2\n",
    "        finds = finds.astype(int)\n",
    "    return finds\n",
    "\n",
    "finds = querybykeys(query)\n",
    "finds = [print(i) for i,j in enumerate(finds) if j]\n",
    "print('Find document id that contains all keys:', finds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find document id that contains all keys: [11848, 21857, 23181, 23516, 25454, 28720, 29154, 45713]\n",
      "房价 白跌 了 ！ 北京 首 套房 贷款 利率 上浮 10%\n",
      "你 认为 北京 未来 5 年 的 房价 会 下跌 吗 ？\n",
      "「 重磅 」 北京 限 房价 项目 销售 办法 出炉   部分 将 收作 共有 产权 房\n",
      "北京 的 房价 还会 继续 上涨 吗 ？\n",
      "北京 房贷利率 下周 再涨   专家 ： 累计 相当于 房价 上调 了 10%\n",
      "北京 限价 房转 共有 产权 房 政策 拟 出炉   对 房价 有何 影响 ？\n",
      "房价 不涨 卖 不动 ， 北京 首 套房 利率 继续 上浮 ， 刚需 招惹 了 谁 ？\n",
      "北京 的 房价 还会 继续 上涨 吗 ？\n"
     ]
    }
   ],
   "source": [
    "# 既然没有那咱找几个小的数据进行继续的说明\n",
    "finds = querybykeys(['北京', '房价']) # ['北京', '房价']\n",
    "finds = [i for i,j in enumerate(finds) if j]\n",
    "print('Find document id that contains all keys:', finds)\n",
    "\n",
    "# 将这些数据打印出来\n",
    "for d in finds:\n",
    "    print(df.iloc[d].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['房价', '白跌', '了', '！', '北京', '首', '套房', '贷款', '利率', '上浮', '10%']\n",
      "{'房价': 0.0909, '北京': 0.0909}\n",
      "0.1818\n",
      "['你', '认为', '北京', '未来', '5', '年', '的', '房价', '会', '下跌', '吗', '？']\n",
      "{'北京': 0.0833, '的': 0.0833, '房价': 0.0833, '？': 0.0833}\n",
      "0.3332\n",
      "['「', '重磅', '」', '北京', '限', '房价', '项目', '销售', '办法', '出炉', '部分', '将', '收作', '共有', '产权', '房']\n",
      "{'北京': 0.0625, '房价': 0.0625}\n",
      "0.125\n",
      "['北京', '的', '房价', '还会', '继续', '上涨', '吗', '？']\n",
      "{'北京': 0.125, '的': 0.125, '房价': 0.125, '？': 0.125}\n",
      "0.5\n",
      "['北京', '房贷利率', '下周', '再涨', '专家', '：', '累计', '相当于', '房价', '上调', '了', '10%']\n",
      "{'北京': 0.0833, '房价': 0.0833}\n",
      "0.1666\n",
      "['北京', '限价', '房转', '共有', '产权', '房', '政策', '拟', '出炉', '对', '房价', '有何', '影响', '？']\n",
      "{'北京': 0.0714, '房价': 0.0714, '？': 0.0714}\n",
      "0.2142\n",
      "['房价', '不涨', '卖', '不动', '，', '北京', '首', '套房', '利率', '继续', '上浮', '，', '刚需', '招惹', '了', '谁', '？']\n",
      "{'房价': 0.0588, '北京': 0.0588, '？': 0.0588}\n",
      "0.1764\n",
      "['北京', '的', '房价', '还会', '继续', '上涨', '吗', '？']\n",
      "{'北京': 0.125, '的': 0.125, '房价': 0.125, '？': 0.125}\n",
      "0.5\n",
      "tf:0.5 检索结果:北京 的 房价 还会 继续 上涨 吗 ？\n"
     ]
    }
   ],
   "source": [
    "# 计算每个数据的TF\n",
    "tf_result = []\n",
    "for d in finds:\n",
    "    words = df.iloc[d].content.split()\n",
    "    # 统计单词出现次数\n",
    "    tf = dict(Counter(words))\n",
    "    # 统计词频\n",
    "    tf = {k:round(v/len(words), 4) for k,v in tf.items() if k in query}\n",
    "    # 统计query的词频\n",
    "    print(words)\n",
    "    print(tf)\n",
    "    print(sum(tf.values()))\n",
    "    tf_result.append(sum(tf.values()))\n",
    "    \n",
    "# 返回所有的权重结果\n",
    "# 找到最大的数据对应的index\n",
    "i = np.argmax(tf_result)\n",
    "# 打印检索后的结果\n",
    "print(f'tf:{max(tf_result)} 检索结果:{df.iloc[finds[i]].content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里面大家有没有发现一个问题？  \n",
    "因为数据量比较少，可能不太明显，  \n",
    "但你还是能够感受得到的。\n",
    "\n",
    "在sim最大的两条数据中，`北京` `房价` `的` 三个单词分别贡献了0.125的权重  \n",
    "但从直观理解来说，`的`字对于问题其实完全没有帮助,因为对于任何问题基本都有`的`字，并且文档中`的`字的频次会更高，这对问答的匹配来说是无效，甚至有害的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么咱们来将我们需要的单词更加明确化一些。  \n",
    "当然，最直接的方法是使用停用词表，将常用的无意义的单词如'的','了'等放入一个词汇表，在检索的关键词中过滤掉这些单词。\n",
    "\n",
    "但这个方法有一定的限制，最简单的停用词表是基本没有问题的。但去掉这些后，对我们检索的结果提升并不会非常大。另外，不同的领域可能拥有不同的停用词，如果对每个任务都手工去\n",
    "创建停用词表，是非常费时费力的。\n",
    "\n",
    "那么有没有一种方法能够帮助我们实现这个功能呢？  \n",
    "\n",
    "其实，我们可以设定两个条件：\n",
    "1. 一个词预测主题的能力越强，那么它对应的权重应该更大，反之越小。比如 `房价`能帮助我们或多或少的了解当前文档的主题。'多少'就不行，那么`房价`的权重应该大于`多少`\n",
    "2. 停用词的权重为零\n",
    "\n",
    "即假定一个关键词$w$在$D_w$个网页中出现过，那么$D_w$越大，$w$的权重越小，反之亦然。  \n",
    "这就是 **逆文本频率** （Inverse Document Frequency，简写为IDF）  \n",
    "$$IDF = \\log \\frac{D}{D_w}$$\n",
    "$D$是全部文档数量。  \n",
    "比如全部文档是1亿，每个文档中都出现了`的`字，那么它的IDF = log(1亿/1亿）=log 1= 0  \n",
    "加入`房价`只出现了20万个网页中，那么它的IDF = log(500) = 8.96  \n",
    "而'多少'出现了5000W个文档中，它的权重IDF = log(2) = 1。\n",
    "\n",
    "所以最终的权重由之前的词频相加变成了：  \n",
    "$$TF_1\\cdot IDF_1 + TF_2\\cdot IDF_2+ ... +TF_N\\cdot IDF_N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，如果你使用了网页排名(pagerank)，那么查询的结果可以由相关性和网页排名的成绩决定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算单词的IDF\n",
    "\n",
    "# 语料\n",
    "corpus = []\n",
    "for document in df.content:\n",
    "    corpus.append(document.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建IDF，以后使用直接调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "for word in vocab:\n",
    "    count = 0\n",
    "    for line in corpus:\n",
    "        if word in line:\n",
    "            count += 1\n",
    "    idf[word] = round(np.log(NUM_DOCUMENTS / count), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 排序简单看下权重大的单词\n",
    "sorted(idf.items(), key=lambda x:x[1]) # 反向 即权重最小的\n",
    "sorted(idf.items(), key=lambda x:x[1], reverse=True) # 正向 权重最大的\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来重构之前的检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有索引，那就遍历每一条数据，将数据中关键字的 计算其tf并乘以IDF 然后求得该数据的最终得分。最后根据得分对数据进行排序。这样计算量很大。\n",
    "\n",
    "现在我们这样，首先根据IDF对查询关键字进行排序，(然后过滤掉过低的关键字)\n",
    "> 由于我们数据较小 多个关键字共现的数据没有 但当数据集特别大的时候 是会存在的 但依然可以使用此方法优化\n",
    "\n",
    "再根据每个关键字索引得到包含关键字的数据子集，然后计算数据的得分，排序，返回结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引找到的数据子集：\n",
      "房价 白跌 了 ！ 北京 首 套房 贷款 利率 上浮 10%\n",
      "你 认为 北京 未来 5 年 的 房价 会 下跌 吗 ？\n",
      "「 重磅 」 北京 限 房价 项目 销售 办法 出炉   部分 将 收作 共有 产权 房\n",
      "北京 的 房价 还会 继续 上涨 吗 ？\n",
      "北京 房贷利率 下周 再涨   专家 ： 累计 相当于 房价 上调 了 10%\n",
      "北京 限价 房转 共有 产权 房 政策 拟 出炉   对 房价 有何 影响 ？\n",
      "房价 不涨 卖 不动 ， 北京 首 套房 利率 继续 上浮 ， 刚需 招惹 了 谁 ？\n",
      "北京 的 房价 还会 继续 上涨 吗 ？\n",
      "\n",
      "word:房价 tf:0.0909 idf:4.3917\n",
      "word:北京 tf:0.0909 idf:5.1568\n",
      "\n",
      "word:北京 tf:0.0833 idf:5.1568\n",
      "word:的 tf:0.0833 idf:1.0274\n",
      "word:房价 tf:0.0833 idf:4.3917\n",
      "word:？ tf:0.0833 idf:0.5814\n",
      "\n",
      "word:北京 tf:0.0625 idf:5.1568\n",
      "word:房价 tf:0.0625 idf:4.3917\n",
      "\n",
      "word:北京 tf:0.125 idf:5.1568\n",
      "word:的 tf:0.125 idf:1.0274\n",
      "word:房价 tf:0.125 idf:4.3917\n",
      "word:？ tf:0.125 idf:0.5814\n",
      "\n",
      "word:北京 tf:0.0833 idf:5.1568\n",
      "word:房价 tf:0.0833 idf:4.3917\n",
      "\n",
      "word:北京 tf:0.0714 idf:5.1568\n",
      "word:房价 tf:0.0714 idf:4.3917\n",
      "word:？ tf:0.0714 idf:0.5814\n",
      "\n",
      "word:房价 tf:0.0588 idf:4.3917\n",
      "word:北京 tf:0.0588 idf:5.1568\n",
      "word:？ tf:0.0588 idf:0.5814\n",
      "\n",
      "word:北京 tf:0.125 idf:5.1568\n",
      "word:的 tf:0.125 idf:1.0274\n",
      "word:房价 tf:0.125 idf:4.3917\n",
      "word:？ tf:0.125 idf:0.5814\n",
      "\n",
      "[0.8679586499999998, 0.92940309, 0.59678125, 1.3946625000000001, 0.79539005, 0.7232748600000001, 0.5956381199999999, 1.3946625000000001]\n",
      "tfidf:1.3946625000000001, data:北京 的 房价 还会 继续 上涨 吗 ？\n"
     ]
    }
   ],
   "source": [
    "# 假设数据子集依然是 ['房价','北京']的数据集\n",
    "print('索引找到的数据子集：')\n",
    "for d in finds:\n",
    "    print(df.iloc[d].content)\n",
    "print()\n",
    "\n",
    "# 计算最后的得分：\n",
    "tfidf_result = []\n",
    "for d in finds:\n",
    "    words = df.iloc[d].content.split()\n",
    "    # 统计单词出现次数\n",
    "    tf = dict(Counter(words))\n",
    "    # 统计词频\n",
    "    tf = {k:round(v/len(words),4) for k,v in tf.items() if k in query}\n",
    "    # 计算TFIDF\n",
    "    tfidf = [v*idf[k] for k,v in tf.items()]\n",
    "    [print(f'word:{k} tf:{v} idf:{idf[k]}') for k,v in tf.items()]\n",
    "    print()\n",
    "    tfidf_result.append(sum(tfidf))\n",
    "\n",
    "# 打印排序结果\n",
    "index = np.argmax(tfidf_result)\n",
    "row_id = finds[index]\n",
    "print(tfidf_result)\n",
    "print(f'tfidf:{max(tfidf_result)}, data:{df.iloc[row_id].content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pagerank算法  \n",
    "- [pagerank介绍](https://blog.csdn.net/hguisu/article/details/7996185)\n",
    "- [pagerank掘金](https://juejin.im/post/5b8ce27ef265da433f2a2f45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索引擎相关项目：\n",
    "- [jorendorff/tinysearch](https://github.com/jorendorff/tinysearch/blob/master/tiny.py)\n",
    "- [trein/simple-search-engine](https://github.com/trein/simple-search-engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
